{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7278c0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Import required modules\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain_core.runnables import RunnableLambda, RunnableMap\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.load import dumps, loads\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from fastapi.responses import StreamingResponse\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import re\n",
    "import os\n",
    "\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68372cd1",
   "metadata": {},
   "source": [
    "Embedding Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfc85403",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3065683/542832890.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(\n",
      "/home/sfnavas-f/Projects/DomainMind/.venv/lib/python3.13/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Embed the text content in split_docs\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"intfloat/e5-base-v2\",\n",
    "    model_kwargs={\"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba40d29c",
   "metadata": {},
   "source": [
    "Load Existing Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0667c1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3065683/360242083.py:2: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(\n"
     ]
    }
   ],
   "source": [
    "# Load the vector store\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=\"../data/doc_vectordb\",\n",
    "    embedding_function=embedding_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57af8257",
   "metadata": {},
   "source": [
    "Retriever Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08eaa3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bdfd77",
   "metadata": {},
   "source": [
    "Setting up Local LLM with Ollama. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d36b65bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3065683/3811996168.py:1: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=\"qwen3:8b\", base_url=\"http://localhost:11434\", streaming=True)\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOllama(model=\"qwen3:8b\", base_url=\"http://localhost:11434\", streaming=True)\n",
    "# llm = ChatOllama(model=\"gemma3:4b\", base_url=\"http://localhost:11434\", streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b73b06",
   "metadata": {},
   "source": [
    "MultiQuery Generation Chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35f58c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "template = \"\"\"You are an AI scientific research assistant. Your task is to generate two \n",
    "different versions of the given user question and the conversation history so as to retrieve relevant documents from a vector \n",
    "database. The vector database consists of scientific papers, articles, books and other academic \n",
    "resources related to the field of Statistical Physics, Computational Physics, Statistics, Soft-Matter Physics \n",
    "and other related fields. By generating multiple perspectives on the user question, your goal is to help \n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. If the below question is unrelated or does not require \n",
    "additional context, you can respond with \"No relevant questions found.\"\n",
    "Original question: {question}\n",
    "Conversation history: {history}\"\"\"\n",
    "multi_prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "def remove_think_blocks(text):\n",
    "    # Remove all <think>...</think> blocks from the text\n",
    "    return re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL).strip()\n",
    "\n",
    "\n",
    "# Query generation cHAIN\n",
    "generate_query_chain = (\n",
    "    multi_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | remove_think_blocks\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db59746",
   "metadata": {},
   "source": [
    "Retrieval Chain for multiple queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f36e02ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make a single list of the retrieved documents\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\"Unique union of retrieved docs\"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "\n",
    "# Retrieval Chain\n",
    "retrieval_chain = generate_query_chain | retriever.map() | get_unique_union"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4421c7e8",
   "metadata": {},
   "source": [
    "# Summary Chain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d4ac8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "template = \"\"\"\n",
    "You are an AI scientific research assistant. Summarize the following conversation between a user and an assistant.\n",
    "- Focus on the key points, main ideas, and any important questions and answers.\n",
    "- Exclude irrelevant, repetitive, or off-topic content.\n",
    "- Use your own words; do not copy the conversation verbatim.\n",
    "- Keep the summary concise and within 512-1024 tokens.\n",
    "- Format the summary as a narration of how the conversation between the user and the assistant unfolded.\n",
    "- If the conversation is empty, respond with \"No conversation history.\"\n",
    "\n",
    "Original conversation:\n",
    "{history}\n",
    "\"\"\"\n",
    "summarization_prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Summarization chain\n",
    "summarization_chain = (\n",
    "    summarization_prompt | llm | StrOutputParser() | remove_think_blocks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a982fe82",
   "metadata": {},
   "source": [
    "Testing the working of the summarization chain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdfb3516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example conversation history\n",
    "history = [\n",
    "    HumanMessage(content=\"What is self-aggregation?\"),\n",
    "    AIMessage(\n",
    "        content=\"Self-aggregation is the process by which molecules or particles spontaneously organize into ordered structures.\"\n",
    "    ),\n",
    "    HumanMessage(content=\"Why is it important in materials science?\"),\n",
    "    AIMessage(\n",
    "        content=\"It's important because it can lead to new material properties and functionalities.\"\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8a609fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: The user initiated a conversation by asking about “self-aggregation.” The AI assistant responded by defining self-aggregation as the spontaneous organization of molecules or particles into ordered structures. Following this explanation, the user then inquired about the significance of self-aggregation within materials science. The AI replied that it's important due to its potential to create materials with novel properties and functionalities. Essentially, the conversation focused on a definition of self-aggregation and its relevance to the development of new materials.\n"
     ]
    }
   ],
   "source": [
    "# See what summary is produced\n",
    "summary = summarization_chain.invoke({\"history\": history})\n",
    "print(\"Summary:\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f90470b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can be used to print the prompt  within the RAG chain\n",
    "def debug_prompt(messages):\n",
    "    print(\"Prompt to LLM:\")\n",
    "    for m in messages:\n",
    "        print(f\"{type(m).__name__}: {getattr(m, 'content', m)}\")\n",
    "    return messages\n",
    "\n",
    "\n",
    "# | RunnableLambda(debug_prompt) Insert this right after the prompt prep and right before the llm call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b24dd4",
   "metadata": {},
   "source": [
    "# RAG Chain \n",
    "\n",
    "Prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cf06ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"\"\"\n",
    "        You are a helpful scientific assistant. You need to answer the question in a scientifically sound manner, \\\\\n",
    "        combining your own knowledge and the  provided context and the conversation history. If you use information \\\\\n",
    "        from the provided context, cite it. If the answer is based on your own knowledge, state so. The conversation \\\\\n",
    "        history is provided above. You also have further context regarding the current question below from snippets \\\\\n",
    "        from various scientific papers. \n",
    "        \"\"\"\n",
    "    ),\n",
    "    HumanMessagePromptTemplate.from_template(\n",
    "        \"Context:\\n{context}\\n\\nQuestion: {question}\\n\\n History: {history}\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "\n",
    "rag_chain_multi_query = (\n",
    "    {\n",
    "        \"history\": RunnableLambda(\n",
    "            lambda x: summarization_chain.invoke({\"history\": x[\"history\"]})\n",
    "        ),\n",
    "        \"context\": RunnableLambda(\n",
    "            lambda x: retrieval_chain.invoke(\n",
    "                {\n",
    "                    \"question\": x[\"question\"],\n",
    "                    \"history\": summarization_chain.invoke({\"history\": x[\"history\"]}),\n",
    "                }\n",
    "            )\n",
    "        ),\n",
    "        \"question\": RunnableLambda(lambda x: (x[\"question\"])),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e070bfaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sfnavas-f/Projects/DomainMind/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "/tmp/ipykernel_3045617/3087458109.py:9: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  return [loads(doc) for doc in unique_docs]\n"
     ]
    }
   ],
   "source": [
    "def split_think_and_answer(response):\n",
    "    \"\"\"Extracts text after </think>.\"\"\"\n",
    "    match = re.search(r\"</think>\\s*(.*)\", response, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return response.strip()\n",
    "\n",
    "\n",
    "def get_thought(response):\n",
    "    \"\"\"Extracts text inside <think>...</think>.\"\"\"\n",
    "    match = re.search(r\"<think>(.*?)</think>\", response, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return None\n",
    "\n",
    "\n",
    "def remove_details_blocks(text):\n",
    "    # Remove all <details>...</details> blocks, including nested tags and multiline content\n",
    "    return re.sub(r\"<details[\\s\\S]*?</details>\", \"\", text, flags=re.DOTALL).strip()\n",
    "\n",
    "\n",
    "def rag_qa_multi_query(message, history):\n",
    "    # Convert Gradio history format to LangChain message format\n",
    "    history_langchain_format = []\n",
    "    # Convert Gradio history (list of {\"role\": ..., \"content\": ...}) to LangChain format\n",
    "    for turn in history:\n",
    "        if turn[\"role\"] == \"user\":\n",
    "            history_langchain_format.append(HumanMessage(content=turn[\"content\"]))\n",
    "        elif turn[\"role\"] == \"assistant\":\n",
    "            cleaned_content = remove_details_blocks(turn[\"content\"])\n",
    "            history_langchain_format.append(AIMessage(content=cleaned_content))\n",
    "    output = \"\"\n",
    "    try:\n",
    "        for chunk in rag_chain_multi_query.stream(\n",
    "            {\"history\": history_langchain_format, \"question\": message}\n",
    "        ):\n",
    "            output += chunk\n",
    "            yield {\"role\": \"assistant\", \"content\": output}\n",
    "\n",
    "        # Pass both current message and history to the chain\n",
    "        # response = rag_chain_multi_query.invoke(\n",
    "        #     {\"history\": history_langchain_format, \"question\": message}\n",
    "        # )\n",
    "        # thought = get_thought(response)\n",
    "        # answer = split_think_and_answer(response)\n",
    "\n",
    "        # if thought:\n",
    "        #     # Add collapsible section with the <think> content\n",
    "        #     final_answer = f\"<details><summary><b>🤔 Thinking</b></summary><pre>{thought}</pre></details>\\n\\n{answer}\"\n",
    "        # else:\n",
    "        #     final_answer = answer\n",
    "\n",
    "        # return {\"role\": \"assistant\", \"content\": final_answer}\n",
    "    except Exception as e:\n",
    "        return {\"role\": \"assistant\", \"content\": f\"❌ Error: {str(e)}\"}\n",
    "\n",
    "\n",
    "# Create and launch the chat interface with memory\n",
    "demo = gr.ChatInterface(\n",
    "    fn=rag_qa_multi_query,\n",
    "    type=\"messages\",\n",
    "    title=\"📄 Scientific PDF Chatbot\",\n",
    "    description=\"Ask questions about your scientific PDFs. Powered by RAG + Qwen3:8B\",\n",
    "    examples=[\"What are colloidal particles?\", \"Tell me more about that\"],\n",
    ")\n",
    "\n",
    "demo.launch(share=False, inline=False, inbrowser=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0169c257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sfnavas-f/Projects/DomainMind/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "/tmp/ipykernel_3065683/3087458109.py:9: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  return [loads(doc) for doc in unique_docs]\n",
      "/home/sfnavas-f/Projects/DomainMind/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "def remove_details_blocks(text):\n",
    "    # Remove all <details>...</details> blocks, including nested tags and multiline content\n",
    "    return re.sub(r\"<details[\\s\\S]*?</details>\", \"\", text, flags=re.DOTALL).strip()\n",
    "\n",
    "\n",
    "def rag_qa_multi_query(message, history):\n",
    "    # Convert Gradio history format to LangChain message format\n",
    "    history_langchain_format = []\n",
    "    # Convert Gradio history (list of {\"role\": ..., \"content\": ...}) to LangChain format\n",
    "    for turn in history:\n",
    "        if turn[\"role\"] == \"user\":\n",
    "            history_langchain_format.append(HumanMessage(content=turn[\"content\"]))\n",
    "        elif turn[\"role\"] == \"assistant\":\n",
    "            cleaned_content = remove_details_blocks(turn[\"content\"])\n",
    "            history_langchain_format.append(AIMessage(content=cleaned_content))\n",
    "    try:\n",
    "        raw_output = \"\"\n",
    "        processed_full_output = \"\"\n",
    "\n",
    "        # Stream the response per token and process <think> tags\n",
    "        # for thinking models\n",
    "        for chunk in rag_chain_multi_query.stream(\n",
    "            {\"history\": history, \"question\": message}\n",
    "        ):\n",
    "            raw_output += chunk\n",
    "\n",
    "            # Check for <think> start\n",
    "            if \"<think>\" in chunk:\n",
    "                chunk = chunk.replace(\"<think>\", \"Reasoning:\")\n",
    "\n",
    "            if \"</think>\" in chunk:\n",
    "                chunk = chunk.replace(\"</think>\", \"End of Reasoning\\n\")\n",
    "\n",
    "            processed_full_output += chunk\n",
    "\n",
    "            yield {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": processed_full_output,\n",
    "            }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"role\": \"assistant\", \"content\": f\"❌ Error: {str(e)}\"}\n",
    "\n",
    "\n",
    "# Create and launch the chat interface with memory\n",
    "demo = gr.ChatInterface(\n",
    "    fn=rag_qa_multi_query,\n",
    "    type=\"messages\",\n",
    "    title=\"📄 Scientific PDF Chatbot\",\n",
    "    description=\"Ask questions about your scientific PDFs. Powered by RAG + Qwen3:8B\",\n",
    "    examples=[\"What are colloidal particles?\", \"Tell me more about that\"],\n",
    ")\n",
    "\n",
    "demo.launch(share=False, inline=False, inbrowser=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b5e4d1",
   "metadata": {},
   "source": [
    "When streaming content from Gradio, < tags > like this can cause issues. If we do not stream and print out the full content at the end, it works fine. Else, when streaming after the first < tag >, the rest of the content is not printed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "domainmind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
